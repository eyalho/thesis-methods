{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing an Autoencoder in PyTorch\n",
    "===\n",
    "\n",
    "This is the PyTorch equivalent of my previous article on implementing an autoencoder in TensorFlow 2.0, which you may read [here](https://towardsdatascience.com/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7)\n",
    "\n",
    "First, to install PyTorch, you may use the following pip command,\n",
    "\n",
    "```\n",
    "$ pip install torch torchvision\n",
    "```\n",
    "\n",
    "The `torchvision` package contains the image data sets that are ready for use in PyTorch.\n",
    "\n",
    "More details on its installation through [this guide](https://pytorch.org/get-started/locally/) from [pytorch.org](pytorch.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our seed and other configurations for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the batch size, the number of training epochs, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 20\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We load our MNIST dataset using the `torchvision` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"flow_mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MNIST\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=10, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6672, 28, 28])\n",
      "torch.Size([6672])\n",
      "torch.Size([6672, 28, 28])\n",
      "torch.Size([6672])\n"
     ]
    }
   ],
   "source": [
    "#FLOW_MNIST\n",
    "\n",
    "if DATASET == \"flow_mnist\":\n",
    "    from flow_mnist_data_set import FlowMnistDataset\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        \n",
    "    train_dataset = FlowMnistDataset(\"ucdavis\", \"up\", 60, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_loader.dataset.train_data = torch.from_numpy(train_loader.dataset.data)\n",
    "    train_loader.dataset.train_labels = torch.from_numpy(train_loader.dataset.labels)\n",
    "\n",
    "    \n",
    "    test_dataset = FlowMnistDataset(\"ucdavis\", \"up\", 60, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "    test_loader.dataset.test_data = torch.from_numpy(test_loader.dataset.data)\n",
    "    test_loader.dataset.test_labels = torch.from_numpy(test_loader.dataset.labels)\n",
    "\n",
    "\n",
    "    print(train_loader.dataset.train_data.shape)\n",
    "    print(train_loader.dataset.train_labels.shape)\n",
    "    print(test_loader.dataset.test_data.shape)\n",
    "    print(test_loader.dataset.test_labels.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6672, 28, 28])\n",
      "torch.Size([6672])\n",
      "torch.Size([6672, 28, 28])\n",
      "torch.Size([6672])\n"
     ]
    }
   ],
   "source": [
    "# Reload flow_mnist_dataset\n",
    "print(train_loader.dataset.train_data.shape)\n",
    "print(train_loader.dataset.train_labels.shape)\n",
    "\n",
    "\n",
    "from flow_mnist_data_set import FlowMnistDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "train_dataset = FlowMnistDataset(\"ucdavis\", \"up\", 60, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "train_loader.dataset.train_data = torch.from_numpy(train_loader.dataset.data)\n",
    "train_loader.dataset.train_labels = torch.from_numpy(train_loader.dataset.labels)\n",
    "\n",
    "print(train_loader.dataset.train_data.shape)\n",
    "print(train_loader.dataset.train_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "An autoencoder is a type of neural network that finds the function mapping the features x to itself. This objective is known as reconstruction, and an autoencoder accomplishes this through the following process: (1) an encoder learns the data representation in lower-dimension space, i.e. extracting the most salient features of the data, and (2) a decoder learns to reconstruct the original data based on the learned representation by the encoder.\n",
    "\n",
    "We define our autoencoder class with fully connected layers for both its encoder and decoder components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_shape\"], out_features=128\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_hidden_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=kwargs[\"input_shape\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.sigmoid(code)\n",
    "        activation = self.decoder_hidden_layer(code)\n",
    "        activation = torch.relu(activation)\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.sigmoid(activation)\n",
    "        return reconstructed, code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using our defined autoencoder class, we have the following things to do:\n",
    "    1. We configure which device we want to run on.\n",
    "    2. We instantiate an `AE` object.\n",
    "    3. We define our optimizer.\n",
    "    4. We define our reconstruction loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eyal/anaconda3/envs/tutorials/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(input_shape=784).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our autoencoder for our specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/20, recon loss = 0.19403388\n",
      "epoch : 2/20, recon loss = 0.04171249\n",
      "epoch : 3/20, recon loss = 0.00390103\n",
      "epoch : 4/20, recon loss = 0.00284401\n",
      "epoch : 5/20, recon loss = 0.00292549\n",
      "epoch : 6/20, recon loss = 0.00283215\n",
      "epoch : 7/20, recon loss = 0.00278988\n",
      "epoch : 8/20, recon loss = 0.00289348\n",
      "epoch : 9/20, recon loss = 0.00271337\n",
      "epoch : 10/20, recon loss = 0.00261435\n",
      "epoch : 11/20, recon loss = 0.00272943\n",
      "epoch : 12/20, recon loss = 0.00268955\n",
      "epoch : 13/20, recon loss = 0.00260074\n",
      "epoch : 14/20, recon loss = 0.00263405\n",
      "epoch : 15/20, recon loss = 0.00257614\n",
      "epoch : 16/20, recon loss = 0.00255395\n",
      "epoch : 17/20, recon loss = 0.00252528\n",
      "epoch : 18/20, recon loss = 0.00256547\n",
      "epoch : 19/20, recon loss = 0.00247758\n",
      "epoch : 20/20, recon loss = 0.00252038\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_loader:\n",
    "        \n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.view(-1, 784).to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        outputs, _ = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    \n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract some test examples to reconstruct using our trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_out.shape torch.Size([2, 128])\n",
      "all_enc_out.shape torch.Size([6672, 128])\n",
      "batch_y.shape torch.Size([2, 1])\n",
      "batch_features.shape torch.Size([2, 28, 28])\n",
      "batch_y.shape torch.Size([2, 1])\n",
      "all_reg_x.shape torch.Size([6672, 28, 28])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_examples = None\n",
    "\n",
    "all_reg_x = []\n",
    "all_enc_out = []\n",
    "all_y = []\n",
    "all_reconstruction = []\n",
    "with torch.no_grad():\n",
    "    for batch_features in test_loader:\n",
    "        batch_y = batch_features[1].view(len(batch_features[1]),1)\n",
    "        batch_features = batch_features[0]\n",
    "        test_examples = batch_features.view(-1, 784)\n",
    "        reconstruction, encoded_out = model(test_examples)\n",
    "        all_enc_out.append(encoded_out)\n",
    "        all_reg_x.append(batch_features.squeeze(1))\n",
    "        all_y.append(batch_y)\n",
    "        all_reconstruction.append(reconstruction)\n",
    "all_enc_out = torch.vstack(tuple(all_enc_out))\n",
    "all_y = torch.vstack(tuple(all_y))\n",
    "all_reg_x = torch.vstack(tuple(all_reg_x))\n",
    "all_reconstruction = torch.vstack(tuple(all_reconstruction))\n",
    "print(\"encoded_out.shape\", encoded_out.shape)\n",
    "print(\"all_enc_out.shape\", all_enc_out.shape)\n",
    "print(\"batch_y.shape\", batch_y.shape)\n",
    "print(\"batch_features.shape\", batch_features.squeeze(1).shape)\n",
    "print(\"batch_y.shape\", batch_y.shape)\n",
    "print(\"all_reg_x.shape\", all_reg_x.shape)\n",
    "\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6672, 784])\n"
     ]
    }
   ],
   "source": [
    "print(all_reconstruction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Let's try to reconstruct some test images using our trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = train_loader.dataset.train_labels\n",
    "    number = len(np.unique(y))\n",
    "    print(number)\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for index in range(number):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, number, index + 1)\n",
    "        plt.imshow(all_reg_x[index].numpy().reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, number, index + 1 + number)\n",
    "        plt.imshow(all_reconstruction[index].numpy().reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6672, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"npy/autoencoder_flow_mnist_X.npy\", all_reg_x.cpu().detach().numpy())\n",
    "np.save(\"npy/autoencoder_flow_mnist_enc_X.npy\", all_enc_out.cpu().detach().numpy())\n",
    "np.save(\"npy/autoencoder_flow_mnist_y.npy\", all_y.cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
